{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdxWwIhHGQht",
        "outputId": "cf76b93c-7364-413f-ef09-acd8ff9a8b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for random\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting Bio\n",
            "  Downloading bio-1.7.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting biopython>=1.80 (from Bio)\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting gprofiler-official (from Bio)\n",
            "  Downloading gprofiler_official-1.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mygene (from Bio)\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Bio) (2.1.4)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from Bio) (1.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Bio) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from Bio) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython>=1.80->Bio) (1.26.4)\n",
            "Collecting biothings-client>=0.2.6 (from mygene->Bio)\n",
            "  Downloading biothings_client-0.3.1-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Bio) (2024.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (4.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch->Bio) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Bio) (2024.7.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.16.0)\n",
            "Downloading bio-1.7.1-py3-none-any.whl (280 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.0/281.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Downloading biothings_client-0.3.1-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: biopython, gprofiler-official, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.7.1 biopython-1.84 biothings-client-0.3.1 gprofiler-official-1.0.0 mygene-3.2.2\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting multiprocessing\n",
            "  Downloading multiprocessing-2.6.2.1.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.8.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading Levenshtein-0.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.25.1 rapidfuzz-3.9.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Bio/pairwise2.py:278: BiopythonDeprecationWarning: Bio.pairwise2 has been deprecated, and we intend to remove it in a future release of Biopython. As an alternative, please consider using Bio.Align.PairwiseAligner as a replacement, and contact the Biopython developers if you still need the Bio.pairwise2 module.\n",
            "  warnings.warn(\n",
            "<ipython-input-1-6a6781bb6af8>:147: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
            "  soundProbabilities = pd.value_counts(soundOccurrences,normalize=True)[sounds]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "!pip install random\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install re\n",
        "!pip install Bio\n",
        "!pip install os\n",
        "!pip install multiprocessing\n",
        "!pip install Levenshtein\n",
        "\n",
        "import random as pyrandom\n",
        "pyrandom.seed(12345)\n",
        "from numpy import *\n",
        "import numpy as np\n",
        "random.seed(12345)\n",
        "import pandas as pd\n",
        "import Levenshtein\n",
        "import re\n",
        "from Bio import pairwise2\n",
        "from multiprocessing import Process,Manager\n",
        "ncores = 50\n",
        "\n",
        "\n",
        "\n",
        "# Function: nexCharOutput\n",
        "# Description:\n",
        "## This function takes a character array, a list of rownames\n",
        "## and the name of the output nexus file as input\n",
        "## and writes the character matrix into a nexus file.\n",
        "## Missing entries are assumed to be coded as \"-1\"\n",
        "\n",
        "def nexCharOutput(chMtx,names,outfile,datatype='STANDARD'):\n",
        "    f = open(outfile,'w')\n",
        "    f.write('#NEXUS\\n\\n')\n",
        "    f.write('BEGIN DATA;\\n')\n",
        "    f.write('DIMENSIONS ntax='+str(len(chMtx))+' NCHAR='+str(len(chMtx.T))+';\\n')\n",
        "    f.write('FORMAT DATATYPE='+datatype+' GAP=? MISSING=- interleave=yes;\\n')\n",
        "    f.write('MATRIX\\n\\n')\n",
        "    txLgth = max(map(len,names))\n",
        "    for i in range(len(chMtx)):\n",
        "        f.write(names[i].ljust(txLgth+2))\n",
        "        for ch in chMtx[i]:\n",
        "            if ch==-1: ch='-'\n",
        "            else:\n",
        "                ch = str(ch)\n",
        "            f.write(ch)\n",
        "        f.write('\\n')\n",
        "    f.write('\\n;\\n\\nEND;\\n')\n",
        "    f.close()\n",
        "\n",
        "\n",
        "data = pd.read_csv('dataset.tab',\n",
        "                   index_col=0,na_filter=False,sep='\\t')\n",
        "data = data[data.wls_gen.isin(['ROMANCE','ALBANIAN'])]\n",
        "data = data[data.index!='LATIN']\n",
        "\n",
        "concepts100 = array(data.columns[9:])\n",
        "nEntries = data[concepts100].apply(lambda x:sum(x!='')).sort_values()\n",
        "concepts = nEntries.index[-40:]\n",
        "data = data[concepts]\n",
        "taxa = array(data.index)\n",
        "\n",
        "def cleanASJP(word):\n",
        "    \"\"\"takes an ASJP string as argument\n",
        "    and returns the string with all diacritics removed.\"\"\"\n",
        "    word = re.sub(r\",|\\%|\\*|\\\"|\\.~|\\$(\\d)|\\s+\", \"\", word)\n",
        "    return word.replace('~', '')\n",
        "\n",
        "\n",
        "dataWL = pd.DataFrame([(c,l,cleanASJP(w))\n",
        "                       for c in concepts\n",
        "                       for l in taxa\n",
        "                       for w in data[c][l].split(',')\n",
        "                       if data[c][l]!=''],\n",
        "                      columns = ['concept','language','word'])\n",
        "\n",
        "dataWL = dataWL.drop_duplicates(['concept','language'])\n",
        "\n",
        "training = pd.DataFrame()\n",
        "for c in concepts:\n",
        "    cData = dataWL[dataWL.concept==c]\n",
        "    cTraining = pd.DataFrame([cData.loc[[i,j]].word.values\n",
        "                              for i in cData.index\n",
        "                              for j in cData.index\n",
        "                              if i<j])\n",
        "    training = pd.concat([training, cTraining], ignore_index=True)\n",
        "\n",
        "sounds = unique(concatenate(list(map(list,dataWL.word.values))))\n",
        "\n",
        "\n",
        "def levalign(w):\n",
        "    \"\"\"takes a pair of strings as input\n",
        "    and returns the Levenshtein alignment\n",
        "    in column format. Gaps are removed.\"\"\"\n",
        "    x,y = w\n",
        "    algn = zeros((0,2))\n",
        "    if '0' in [x,y]: return algn\n",
        "    e = Levenshtein.opcodes(x,y)\n",
        "    for a in e:\n",
        "        if a[0] in ['replace','equal']:\n",
        "            x_a, x_e = a[1],a[2]\n",
        "            y_a, y_e = a[3],a[4]\n",
        "            ag = [list(x[x_a:x_e]), list(y[y_a:y_e])]\n",
        "            algn = concatenate([algn,transpose(ag)])\n",
        "    return algn\n",
        "\n",
        "\n",
        "manager = Manager()\n",
        "return_dict = manager.dict()\n",
        "packages = array_split(training.values,ncores)\n",
        "\n",
        "\n",
        "def doWork(i,pck):\n",
        "    return_dict[i] = vstack([levalign(p) for p in pck])\n",
        "\n",
        "\n",
        "jobs = []\n",
        "for i,pck in enumerate(packages):\n",
        "    p = Process(target=doWork,args=(i,pck))\n",
        "    p.start()\n",
        "    jobs.append(p)\n",
        "for p in jobs:\n",
        "    p.join()\n",
        "alg0 = vstack(return_dict.values())\n",
        "\n",
        "\n",
        "# count alignment frequencies\n",
        "sFreqs = pd.crosstab(alg0[:,0],alg0[:,1])\n",
        "sFreqs = sFreqs.reindex(sounds,fill_value=0).T\n",
        "sFreqs = sFreqs.reindex(sounds,fill_value=0).T\n",
        "# symmetrize\n",
        "sFreqs = sFreqs.copy()+sFreqs.copy().T\n",
        "# add-1 smoothing\n",
        "sFreqs += 1\n",
        "sProbs = sFreqs/sFreqs.sum().sum()\n",
        "\n",
        "# extract relative sound frequencies\n",
        "soundOccurrences = concatenate([list(w) for w in dataWL.word.values])\n",
        "soundProbabilities = pd.value_counts(soundOccurrences,normalize=True)[sounds]\n",
        "pmi0 = (log(sProbs).copy()-log(soundProbabilities)).T-log(soundProbabilities)\n",
        "\n",
        "\n",
        "def sscore(a,b,pmiDict,gp1,gp2):\n",
        "    \"\"\"a,b: ASJP strings\n",
        "    pmiDict: logodds dictionary\n",
        "    gp1,gp2: gap penalties\n",
        "    return PMI score of a/b\n",
        "    \"\"\"\n",
        "    out = pairwise2.align.globalds(a,b,pmiDict,gp1,gp2)\n",
        "    if len(out)==0: return nan\n",
        "    return out[0][2]\n",
        "\n",
        "\n",
        "def scoreNW(x,y,pmiDict,gp1,gp2):\n",
        "    \"\"\"x,y: sequences of ASJP strings, separated by '-'\n",
        "    pmiDict: logodds dictionary\n",
        "    gp1,g2: gap penalties\n",
        "    returns maximal PMI score for the Cartesian product of x and y\"\"\"\n",
        "    if '0' in [x,y]: return nan\n",
        "    x1=x.split('-')\n",
        "    y1=y.split('-')\n",
        "    return max([sscore(xx,yy,pmiDict,gp1,gp2) for xx in x1 for yy in y1])\n",
        "\n",
        "\n",
        "pmi0Dict = {(s1,s2):pmi0[s1][s2]\n",
        "            for s1 in sounds for s2 in sounds}\n",
        "\n",
        "\n",
        "# def nw(x,y,pmiDict,gp1,gp2):\n",
        "#     \"\"\"wrapper for Bio.pairwise2.align.globalds\"\"\"\n",
        "#     return pairwise2.align.globalds(x,y,pmiDict,gp1,gp2)\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "# def nwalign(w,pmiDict,gp1,gp2,th=-Inf):\n",
        "#     \"\"\"w: a pair of ASJP strings\n",
        "#     pmiDict: dictionary of logodds (=PMI scores)\n",
        "#     gp1,gp2: gap penalties (non-positive)\n",
        "#     th: threshold; all pairs with a PMI-score <th will be ignored\n",
        "#     returns: array of pairwise alignment, with gaps removed\"\"\"\n",
        "#     x,y = w\n",
        "#     a = nw(x,y,pmiDict,gp1,gp2)\n",
        "#     if len(a)==0: return zeros((0,2))\n",
        "#     algn = []\n",
        "#     if a[0][2]<th:\n",
        "#         return zeros((0,2))\n",
        "#     for aa in a:\n",
        "#         l = len(aa[0])\n",
        "#         aaa = [[aa[0][i],aa[1][i]] for i in range(l)]\n",
        "#         algn += [x for x in aaa if not '-' in x]\n",
        "#     return array(algn)\n",
        "#\n",
        "# def nwalignStar(crp,pmiDict,gp1,gp2,th):\n",
        "#     packages = array_split(crp,ncores)\n",
        "#     manager = Manager()\n",
        "#     return_dict = manager.dict()\n",
        "#     def doWork(i,pck):\n",
        "#         return_dict[i] = vstack([nwalign(w,pmiDict,gp1,gp2,th)\n",
        "#                                  for w in pck])\n",
        "#     jobs = []\n",
        "#     for i,pck in enumerate(packages):\n",
        "#         p = Process(target=doWork,args=(i,pck))\n",
        "#         p.start()\n",
        "#         jobs.append(p)\n",
        "#     for p in jobs:\n",
        "#         p.join()\n",
        "#     return vstack(return_dict.values())\n",
        "\n",
        "gp1=-2.49302792222\n",
        "gp2=-1.70573165621\n",
        "th = 4.4451\n",
        "\n",
        "def iwsa(a, b, w, IL):\n",
        "    m, n = len(a), len(b)\n",
        "    M = np.zeros((m + 1, n + 1))\n",
        "    for i in range(1, m + 1):\n",
        "        M[i][0] = M[i - 1][0] + w(a[i - 1], '-') * IL(a, b, i - 1, '-')\n",
        "    for j in range(1, n + 1):\n",
        "        M[0][j] = M[0][j - 1] + w('-', b[j - 1]) * IL(a, b, '-', j - 1)\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            M[i][j] = min(\n",
        "                M[i - 1][j - 1] + w(a[i - 1], b[j - 1]) * IL(a, b, i - 1, j - 1),\n",
        "                M[i - 1][j] + w(a[i - 1], '-') * IL(a, b, i - 1, '-'),\n",
        "                M[i][j - 1] + w('-', b[j - 1]) * IL(a, b, '-', j - 1)\n",
        "            )\n",
        "    return M[m][n]\n",
        "\n",
        "def iwsa_score(x, y, pmiDict, gp1, gp2):\n",
        "    if '0' in [x, y]:\n",
        "        return nan\n",
        "    x1 = x.split('-')\n",
        "    y1 = y.split('-')\n",
        "    return max([iwsa(xx, yy, lambda a, b: pmiDict.get((a, b), -inf), lambda a, b, i, j: 1) for xx in x1 for yy in y1])\n",
        "\n",
        "def iwsa_align(w, pmiDict, gp1, gp2, th):\n",
        "    x, y = w\n",
        "    a = iwsa(x, y, lambda a, b: pmiDict.get((a, b), -inf), lambda a, b, i, j: 1)\n",
        "    if a < th:\n",
        "        return zeros((0, 2))\n",
        "    algn = []\n",
        "    for i in range(len(x)):\n",
        "        for j in range(len(y)):\n",
        "            if x[i] != '-' and y[j] != '-':\n",
        "                algn.append([x[i], y[j]])\n",
        "    return array(algn)\n",
        "\n",
        "def iwsa_align_star(crp, pmiDict, gp1, gp2, th):\n",
        "    packages = array_split(crp, ncores)\n",
        "    manager = Manager()\n",
        "    return_dict = manager.dict()\n",
        "    def doWork(i, pck):\n",
        "        try:\n",
        "            result = vstack([iwsa_align(w, pmiDict, gp1, gp2, th) for w in pck])\n",
        "            return_dict[i] = result\n",
        "        except Exception as e:\n",
        "            print(f\"Error in process {i}: {e}\")\n",
        "    jobs = []\n",
        "    for i, pck in enumerate(packages):\n",
        "        p = Process(target=doWork, args=(i, pck))\n",
        "        p.start()\n",
        "        jobs.append(p)\n",
        "    for p in jobs:\n",
        "        p.join()\n",
        "    if return_dict:\n",
        "        return vstack(return_dict.values())\n",
        "    else:\n",
        "        print(\"Warning: No results returned from child processes.\")\n",
        "        return np.array([])\n",
        "\n",
        "gp1=-2.49302792222\n",
        "gp2=-1.70573165621\n",
        "th = 4.4451\n",
        "\n",
        "# for i in range(10):\n",
        "#     alg = nwalignStar(training.values,pmiDict,gp1,gp2,th)\n",
        "#     sFreqs = pd.crosstab(alg[:,0],alg[:,1])\n",
        "#     sFreqs = sFreqs.reindex(sounds,fill_value=0).T\n",
        "#     sFreqs = sFreqs.reindex(sounds,fill_value=0).T\n",
        "#     sFreqs = sFreqs.copy()+sFreqs.copy().T\n",
        "#     sFreqs += 1\n",
        "#     sProbs = sFreqs/sFreqs.sum().sum()\n",
        "#     pmi = (log(sProbs).copy()-log(soundProbabilities)).T-log(soundProbabilities)\n",
        "#     pmiDict = {(s1,s2):pmi[s1][s2]\n",
        "#                for s1 in sounds for s2 in sounds}\n",
        "\n",
        "pmiDict = pmi0Dict.copy()\n",
        "for i in range(10):\n",
        "    alg = iwsa_align_star(training.values, pmiDict, gp1, gp2, th)\n",
        "    sFreqs = pd.crosstab(alg[:, 0], alg[:, 1])\n",
        "    sFreqs = sFreqs.reindex(sounds, fill_value=0).T\n",
        "    sFreqs = sFreqs.reindex(sounds, fill_value=0).T\n",
        "    sFreqs = sFreqs.copy() + sFreqs.copy().T\n",
        "    sFreqs += 1\n",
        "    sProbs = sFreqs / sFreqs.sum().sum()\n",
        "    pmi = (log(sProbs).copy() - log(soundProbabilities)).T - log(soundProbabilities)\n",
        "    pmiDict = {(s1, s2): pmi[s1][s2]\n",
        "               for s1 in sounds for s2 in sounds}\n",
        "\n",
        "pmi.to_csv('pmi-albanoRomance_IWSA.csv')\n",
        "dataWL.to_csv('albanoRomanceASJP_IWSA.csv',index=False)\n",
        "\n",
        "sc = pd.DataFrame(index=taxa)\n",
        "for c in concepts:\n",
        "    cData = dataWL[dataWL.concept==c]\n",
        "    cTaxa = cData.language.unique()\n",
        "    cWords = pd.Series([''.join(cData[cData.language==l].word.values)\n",
        "                        for l in cTaxa],\n",
        "                       index=cTaxa)\n",
        "    cMtx = pd.DataFrame([[int(s in cWords[l]) for s in sounds]\n",
        "                         for l in cTaxa],\n",
        "                        index=cTaxa)\n",
        "    cMtx = cMtx.reindex(taxa,fill_value='-')\n",
        "    sc = pd.concat([sc,cMtx],axis=1)\n",
        "\n",
        "nexCharOutput(sc.values,sc.index,'albanoRomanceSC_IWSA.nex','restriction')\n",
        "\n",
        "def levalignFull(w):\n",
        "    \"\"\"takes a pair of strings as input\n",
        "    and returns the Levenshtein alignment\n",
        "    in column format.\"\"\"\n",
        "    x,y = w\n",
        "    algn = zeros((0,2))\n",
        "    e = Levenshtein.opcodes(x,y)\n",
        "    for a in e:\n",
        "        if a[0] in ['replace','equal']:\n",
        "            x_a, x_e = a[1],a[2]\n",
        "            y_a, y_e = a[3],a[4]\n",
        "            ag = [list(x[x_a:x_e]), list(y[y_a:y_e])]\n",
        "        elif a[0]=='delete':\n",
        "            x_a, x_e = a[1],a[2]\n",
        "            ag = [list(x[x_a:x_e]), ['-']*(x_e-x_a)]\n",
        "        else:\n",
        "            y_a, y_e = a[3],a[4]\n",
        "            ag = [ ['-']*(y_e-y_a),list(y[y_a:y_e])]\n",
        "        algn = concatenate([algn,transpose(ag)])\n",
        "    return algn\n"
      ]
    }
  ]
}